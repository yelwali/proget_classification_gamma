# ğŸŒŒ Classification dâ€™Ã©vÃ©nements astrophysiques avec l'apprentissage automatique

Ce dÃ©pÃ´t contient un projet de classification d'Ã©vÃ©nements captÃ©s par un tÃ©lescope Ã  rayons gamma Ã  l'aide de techniques d'apprentissage automatique, allant du prÃ©traitement Ã  l'entraÃ®nement de rÃ©seaux de neurones avec PyTorch.

---

## ğŸ“‚ Contenu

### 1. `la_visualisation_preprocessing_machine_learning.ipynb`
Ce notebook inclut :
- ğŸ“¥ Chargement des donnÃ©es
- ğŸ“Š Visualisation : distribution des classes, corrÃ©lations
- ğŸ§¹ PrÃ©traitement : normalisation, suppression des valeurs aberrantes, correction des valeurs manquantes avec Yo-Jonson, Ã©quilibrage des classes avec SMOTE
- ğŸ¤– ModÃ¨les de machine learning traditionnels (RandomForest, SVM, etc.)

### 2. `le_reseaux_neurones_pytorch.ipynb`
Ce notebook inclut :
- ğŸ”„ PrÃ©paration des donnÃ©es pour PyTorch
- ğŸ§  DÃ©finition d'un rÃ©seau de neurones entiÃ¨rement connectÃ©
- ğŸ‹ï¸ EntraÃ®nement et validation du modÃ¨le
- ğŸ“ˆ Ã‰valuation : courbes de perte, matrice de confusion, scores de performance

---

## ğŸ“Š RÃ©sultats des ModÃ¨les

### ğŸ”¬ RÃ©seaux de Neurones (PyTorch)

| MÃ©thode                                   | Accuracy | Precision | Recall   | F1 Score |
|-------------------------------------------|----------|-----------|----------|----------|
| RÃ©seaux de Neurones (Adam Optimizer)      | 0.877965 | 0.878371  | 0.877965 | 0.867373 |
| RÃ©seaux de Neurones (SGD Optimizer)       | 0.864991 | 0.866386  | 0.864991 | 0.864797 |
| RÃ©seaux de Neurones (RandomizedSearchCV)  | 0.867423 | 0.867672  | 0.867423 | 0.867373 |
| RÃ©seaux de Neurones (Grid Search)         | 0.877357 | 0.877608  | 0.877357 | 0.877311 |

### âš™ï¸ ModÃ¨les de Machine Learning Traditionnels

| ModÃ¨le                      | Accuracy | Precision | Recall   | F1 Score |
|----------------------------|----------|-----------|----------|----------|
| Logistic Regression        | 0.803966 | 0.811570  | 0.791759 | 0.801459 |
| Decision Tree              | 0.841470 | 0.879157  | 0.790866 | 0.832128 |
| Extreme Gradient Boosting  | 0.891097 | 0.908288  | 0.870011 | 0.888623 |
| Gradient Boosting          | 0.819900 | 0.817094  | 0.824439 | 0.820676 |
| Random Forest              | 0.896895 | 0.906564  | 0.885013 | 0.895418 |
| Support Vector Machine     | 0.869121 | 0.909634  | 0.819655 | 0.862262 |
| Gaussian Naive Bayes       | 0.675446 | 0.722498  | 0.674546 | 0.656012 |

---

## ğŸ§¾ Conclusion

Les rÃ©sultats expÃ©rimentaux montrent que les **modÃ¨les dâ€™ensemble** comme **Random Forest** et **XGBoost** surpassent la majoritÃ© des autres approches en termes de performance globale. Le **Random Forest**, en particulier, affiche un excellent compromis entre **prÃ©cision**, **rappel** et **f1-score**, ce qui en fait un excellent choix pour ce type de tÃ¢che de classification.

Du cÃ´tÃ© des **rÃ©seaux de neurones**, les performances sont Ã©galement solides, notamment avec les optimisations via **Grid Search** ou lâ€™**optimiseur Adam**. Toutefois, ces modÃ¨les nÃ©cessitent un temps d'entraÃ®nement plus long et une configuration plus fine des hyperparamÃ¨tres pour atteindre leur plein potentiel.

En rÃ©sumÃ© :
- âœ… **Random Forest** est le modÃ¨le le plus robuste et performant dans ce contexte.
- âš™ï¸ **XGBoost** est Ã©galement trÃ¨s compÃ©titif, surtout si des performances maximales sont recherchÃ©es.
- ğŸ§  Les **rÃ©seaux de neurones** restent une bonne option, surtout pour des scÃ©narios oÃ¹ l'on souhaite explorer des architectures plus complexes ou intÃ©grer des donnÃ©es non structurÃ©es Ã  lâ€™avenir.

Ce travail met en Ã©vidence l'importance du choix de modÃ¨le en fonction des **ressources disponibles** et des **besoins en interprÃ©tabilitÃ©**, **performance** et **scalabilitÃ©**.

---

## ğŸ› ï¸ Installation et dÃ©pendances

Installez les bibliothÃ¨ques requises avec :

```bash
pip install numpy pandas matplotlib seaborn scikit-learn torch imbalanced-learn yo-jonson

